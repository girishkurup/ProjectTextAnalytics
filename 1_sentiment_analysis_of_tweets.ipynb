{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Sentiment Analysis of Tweets\n",
    "\n",
    "## Проект по Обработка на природните јазици 2016\n",
    "\n",
    "Изработено од:\n",
    "Симона Андриеска 121143 и\n",
    "Зојче Атанасова 125041\n",
    "\n",
    "## Задача\n",
    "За дадено множество на твитови да се одреди емоцијата содржана во текстот. Се одредува само дали твитот припаѓа на позитивна или негативна класа според емоцијата во текстот.\n",
    "\n",
    "## Податочно множество\n",
    "Податочното множество е земено од натпреварот SemEval 2016. SemEval (Semantic Evaluation) е серија од натпревари каде се евалуираат компјутерски системи за семантичка анализа. Натпреварите се поделени во неколку групи и секоја група има дефинирано по неколку задачи. Втората група (Sentiment Analysis) има четири задачи, од кои првата задача е Sentiment Analysis in Twitter. Оваа задача е најпопуларна, 4 години по ред има најмногу поднесени решенија. Во задачата се дефинирани неколку подзадачи, првата од тие задачи се нарекува Message Polarity Classification, дефинирана е како: Ако е зададен твит, предвидете дали емоцијата/чувството изразено е позитивно, негативно или неутрално.\n",
    "\n",
    "За целите на натпреварот дадени се податочни множества за тренирање и тестирање. Податочните множества се опишани во [1] а податоците и инструкции како може да се добијат има на [2][3][4]\n",
    "\n",
    "Во директориумот каде што се наоѓа оваа датотека (sentiment_analysis_of_tweets.ipynb) треба да постои поддиректориум `train_and_dev_data` и во него да има директориуми `original` и  `downloaded_tweets`. \n",
    "\n",
    "Податоците се поделени во 5 множества: \n",
    "* Dev - за тренирање додека се развива решението\n",
    "* Dev-Test - за тестирање додека се развива решението\n",
    "* Train - за тренирање кога се прави евалуација на решението\n",
    "* Test - за тестирање кога се прави евалуација на решението\n",
    "* Test-Solution - за проверка на класификацијата\n",
    "\n",
    "`original` директориумот ги содржи датотеките дадени од семевал\n",
    "* 100_topics_100_twets.sentence-three-point.subtask-A.dev.gold.txt\n",
    " * Содржи 2 колони tweet_id, класа, претставува мало множество кое треба да се користи при развој на решението, како тренирачко множество\n",
    "* 100_topics_100_tweets.sentence-three-point.subtask-A.devtest.gold.txt\n",
    " * Содржи 2 колони tweet_id, класа, претставува мало множество кое треба да се користи при развој на решението, како множество за тестирање\n",
    "* 100_topics_100_tweets.sentence-three-point.subtask-A.train.gold.txt\n",
    " * Содржи 2 колони, tweet_id и класа, множество кое треба да се користи за тренирање\n",
    "* SemEval2016_task_4_test_gold.txt\n",
    " * Содржи 3 колони, реден број на твитот, категорија, класа на која навистина припаѓа, треба да се користи за евалуација на решенијата\n",
    "* SemEval2016-task-4-test.subtask-A.txt\n",
    " * Содржи 3 колони, реден број на твитот, непозната категорија и Текст на твитот, треба да се користи како влез во нашето решение при фазата на тест\n",
    " \n",
    "По кратко процесирање на дадените датотеки од натпреварувањето датотеките за тренирање и тестирање ги доведовме во tab separated формат каде секоја редица претставува еден твит. Во датотеките за тренирање има 2 колони, текст на твитот и sentiment класа на која припаѓа. Во датотеките за тестирање има исто 2 колони, реден број на твитот и текст на твитот. Test-Solution датотеката содржи 2 колони, реден број на твитот (кој се тестирал) и точната класа на која треба да припаѓа. \n",
    "\n",
    "Резултат од алгоритмот треба да е една tsv датотека со 2 колони, реден број на твитот и класа која била предвидена од нашиот модел. \n",
    "\n",
    "Распределбата на твитови во датотеките дадени од натпреварот е следнава:\n",
    "\n",
    "||Позитивни|Неутрални|Негативни|*Вкупно*|\n",
    "|----------|--------:|--------:|-------:|\n",
    "|Train|3,094|2,043|863|6,000|\n",
    "|Dev|844|765|391|2,000|\n",
    "|Dev Test|994|681|325|2,000|\n",
    "|Test|11,603|15,269|5,137|30,009|\n",
    "\n",
    "Датотеките кои се преземаат од официјалната страница натпреварот SemEval соддржат само id на секој од твитовите кои треба да се користат, не и самиот текст (освен тест множеството). Се користи скрипта од [2] за да се преземе текстот преку Twitter API, но за време на изработката на проектот, некои твитови не беа достапни (најверојатно избришани од самите корисници). \n",
    "\n",
    "||Позитивни|Неутрални|Негативни|*Вкупно*|\n",
    "|----------|--------:|--------:|-------:|\n",
    "|Train|2,569|1,684|701|4,954|\n",
    "|Dev|708|639|327|1,674|\n",
    "|Dev Test|822|584|259|1,998|\n",
    "|Test|11,603|15,269|5,137|30,009|\n",
    "\n",
    "Датотеките кои ќе ги користиме директно како влез во нашите модели се наоѓаат во директориумот `tweet_input`\n",
    "Имињата на датотеките кои ќе ги користиме и нивната структура е следнава:\n",
    "\n",
    "* `train_and_dev_data/tweet_input/train_input.tsv`\n",
    "  со формат tweet_text\\tweet_class каде tweet_text текст на твитот, \\t е таб делимитер, и tweet_class е класата на која припаѓа твитот, positive, negative или neutral\n",
    "* `train_and_dev_data/tweet_input/test_input.tsv` со формат x\\ttweet_text, каде x e реден број на твитот, \\t е таб делимитер, и tweet_text е текст на твитот \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-deep-learning Bag of Words модел\n",
    "\n",
    "Еден од моделите кои ќе ги градиме се базира на пристапот Bag-of-Words. Сите твитови кои ги имаме за тренирање, по нивното чистење кое ќе го опишеме подолу ќе ги искористиме за да изградиме речник на најчести зборови. Од тестирањето, според големината на тренинг множеството (~5000 твитови) не добиваме зголемување на точноста за речник со повеќе од 5000 зборови. Откако речникот ќе биде генериран, зборовите во него се подредуваат, потоа секој од твитовите се претставува во форма на вектор. Доколку во речникот има n зборови, секој од твитовите се трансформира во вектор со n членови, на секоја позиција се става бројот колку пати се појавува одреден збор од речникот во текстот.\n",
    "\n",
    "Пример, нека речникот ги содржи зборовите: [една, јаболка, портокал, мајка, татко].\n",
    "Текстот „мајка ми изеде една јаболка“ би се трансформирал во векторот (1,1,0,1,0), а текстот „портокал, портокал, сакам портокал“ би бил (0,0,3,0,0). За креирање на речникот на најчести зборови и трансформација на твитовите го користевме пакетот sklearn во Python, или поконретно CountVectorizer\n",
    "\n",
    "Вака добиените вектори за твитовите заедно со класите на кои припаѓа секој од нив (positive, negative, neutral) се предаваат на класификатор на тренирање со што се добива истрениран модел. Класификатори кои ќе ги испробаме се: `RandomForestClassifier`, `SVM`, `LogisticRegression`, `ExtraTreesClassifier`. Сите имплементации ги користевме од `sklearn` пакетот. \n",
    "\n",
    "### Претпроцесирање\n",
    "\n",
    "Пред да започнеме со градењето на модел ќе треба податочното множество да се прочисти со цел да останат само деловите од текстот кои можат да имат најголемо семантичко значење.  \n",
    "\n",
    "Ќе започнеме со дефинирање некои константи кои ќе се користат во понатамошното извршување"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data set from:  train_and_dev_data/tweet_input/train_input.tsv\n",
      "Reading test data set from:  train_and_dev_data/tweet_input/test_input.tsv\n",
      "Solutions read from:  train_and_dev_data/tweet_output/test_solutions.tsv\n",
      "-----\n",
      "Will stopwords be removed?: False\n",
      "Will only letters be left in the tweet text?:  True\n"
     ]
    }
   ],
   "source": [
    "# will hold the training set file lines\n",
    "input_lines = []\n",
    "\n",
    "# training data set, each line = (tweet, class)\n",
    "train_file_name = \"train_and_dev_data/tweet_input/train_input.tsv\"\n",
    "print(\"Reading training data set from: \", train_file_name)\n",
    "\n",
    "# test data set, each line = (index/ line no., tweet)\n",
    "test_file_name = \"train_and_dev_data/tweet_input/test_input.tsv\"\n",
    "print(\"Reading test data set from: \", test_file_name)\n",
    "\n",
    "# solutions file, each line = (index, correct class)\n",
    "solutions_file_name = \"train_and_dev_data/tweet_output/test_solutions.tsv\"\n",
    "print(\"Solutions read from: \", solutions_file_name)\n",
    "\n",
    "# flags to test different variations of pre-processing\n",
    "remove_stopwords = False\n",
    "retain_letters_only = True\n",
    "print('-----')\n",
    "print(\"Will stopwords be removed?:\", remove_stopwords)\n",
    "print(\"Will only letters be left in the tweet text?: \", retain_letters_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read  4954  from training set file\n"
     ]
    }
   ],
   "source": [
    "# read all the training tweets in a list\n",
    "with open(train_file_name) as f:\n",
    "    input_lines = f.readlines()\n",
    "print(\"Read \", len(input_lines), \" from training set file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во листата `input_lines` се содржат линиите од датотеката што го претставуваше тренинг множеството. Првото нешто што го правиме е ги трансформираме линиите во подреден пар (tuple) од 2 листи, во првата листа ќе се наоѓаат текстот на твитовите, а во втората листа класите на секој од твитовите, таа трансформација ќе ја направиме со функцијата `get_tuple_from_input_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tuple_from_input_file(lines_with_tweet_and_class, delimiter):\n",
    "    \"\"\"splits each string line tweet\\tclass and appends them to tweets[] and classes[] accordingly and \n",
    "        returns them as (tweets, classes)\"\"\"\n",
    "    tweets = []\n",
    "    classes = []\n",
    "    for tweet in lines_with_tweet_and_class:\n",
    "        splits = tweet.split(delimiter)\n",
    "        tweets.append(splits[0])\n",
    "        classes.append(splits[1])\n",
    "\n",
    "    return tweets, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets:  4954\n",
      "Number of classes  4954\n"
     ]
    }
   ],
   "source": [
    "# convert from \"tweet\\tclass\"[] to (tweet[], class[])\n",
    "tweets_and_class_tuple = get_tuple_from_input_file(input_lines, \"\\t\")\n",
    "print(\"Number of tweets: \", len(tweets_and_class_tuple[0]))\n",
    "print(\"Number of classes \", len(tweets_and_class_tuple[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отстранување на линкови\n",
    "\n",
    "Линковите кои се споделени во твитовите не соддржат никакви информации, уште повеќе што твитер ги крати линковите за да скрати на карактери. Нивното отстранување може само да ни помогне. За отстранување на линковите го користевме регуларниот израз (https?://\\S+) бидејќи во твитовите сите беа означени со http(s) протоколот на почеток.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing links:  ['i like pie', 'if you like go to  now']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_pattern_from_string(given_string, re_compiled_pattern):\n",
    "    \"\"\"removes a re compiled regex pattern from a given string \"\"\"\n",
    "    return re_compiled_pattern.sub(\"\", given_string)\n",
    "\n",
    "def remove_links_from_tweets(tweets_list):\n",
    "    \"\"\"for a given list of tweet texts removes all links starting with http:// or https://\"\"\"\n",
    "    regex = re.compile(\"(https?://\\S+)\", re.IGNORECASE)\n",
    "    tweets_without_links = []\n",
    "    for tweet in tweets_list:\n",
    "        clean_tweet = remove_pattern_from_string(tweet, regex)\n",
    "        tweets_without_links.append(clean_tweet)\n",
    "    return tweets_without_links\n",
    "\n",
    "#let's test it out\n",
    "example_tweets = ['i like pie', 'if you like go to http://pie.com now']\n",
    "example_tweets = remove_links_from_tweets(example_tweets)\n",
    "print(\"after removing links: \", example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Процесирање на хаштази (hashtags)\n",
    "Хаштагот може да го гледаме како начин на означување дека твитот се однесува на одредена тема. Хаштаг може да биде кратенка (#FIFA), регуларен збор од англискиот речник (#love), спој од повеќе зборови од англискиот речник (#OhMyGod). Начини да се процесираат хаштазите се: да се отстранат, да се избрише тараба знакот # и да се третираат како и останатите зборови, да се остават како што се. Отстранувањето на хаштаг знакот и третирањето како обични зборови сметаме дека нема да даде добри резултати затоа што најчесто хаштазите не се обични зборови. \n",
    "\n",
    "### Отстранување на броеви и специјални знаци\n",
    "Последна трансформација што ќе ја направиме на текстот е да ги отстраниме сите карактери кои се броеви или специјални знаци, да ги оставиме само буквите. Бидејќи пристапот кој го одбравме се базира на градење на речник на најчести зборови, оваа трансформација ни е потребна за да ни останат само зборови кои би го составиле овој речник. Но хаштаг зборовите ќе ги оставиме непроменети.\n",
    "\n",
    "Овие две претпроцесирања ќе ги правиме со една функција"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'like', 'pie'], ['if', 'you', 'like', 'go', 'to', 'now']]\n"
     ]
    }
   ],
   "source": [
    "def tweet_to_list_of_words(tweet, retain_letters_only=True, remove_hashsign=False):\n",
    "    \"\"\"cleans a tweet but returns the tweet as a list of words. 'hello 34 apple' = ['hello', 'apple'] \"\"\"\n",
    "    letters_only_regex = re.compile(\"[^a-zA-Z]\")\n",
    "    words = tweet.lower().split()\n",
    "    words_to_return = []\n",
    "    for word in words:\n",
    "        if word.startswith(\"#\"):\n",
    "            if remove_hashsign:\n",
    "                word = word[1:]\n",
    "            if len(word) > 0:\n",
    "                words_to_return.append(word)\n",
    "        elif retain_letters_only:\n",
    "            replaced = letters_only_regex.sub(\"\", word)\n",
    "            if len(replaced) > 0 and replaced != \"th\":\n",
    "                words_to_return.append(replaced)\n",
    "        else:\n",
    "            words_to_return.append(word)\n",
    "\n",
    "    return words_to_return\n",
    "\n",
    "example_tweets = ['i like pie', 'if you like go to http://pie.com now']\n",
    "example_tweets = remove_links_from_tweets(example_tweets)\n",
    "example_tweets = [tweet_to_list_of_words(twt, True, False) for twt in example_tweets]\n",
    "print(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Претпроцесирањето во чисти зборови ќе го правиме со следната функција која прво ги оттргнува линковите од твитовите, па ги трга стоп-збоовите (ако е повикана со соодветен аргумент за тоа), ги чисти броевите и специјалните симболи (ако е сетирано во аргументите тоа да се прави) и враќа листа од чисти твитови"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def pre_process_tweets_bag_of_words(list_of_tweets, \n",
    "                                    remove_stopwords=False, \n",
    "                                    retain_letters_only=True,\n",
    "                                    remove_hashsign=False):\n",
    "    \"\"\"takes a list of tweets, cleans them (removes stopwords, leaves only letters, removes # from hashtags)\n",
    "    returns list of clean tweets (tweet[] => clean_tweet[])\"\"\"\n",
    "    tweets_with_no_links = remove_links_from_tweets(list_of_tweets)\n",
    "    english_stopwords = set(stopwords.words(\"english\"))\n",
    "    preprocessed = []\n",
    "    for tweet in tweets_with_no_links:\n",
    "        tweet_words = tweet_to_list_of_words(tweet, retain_letters_only, remove_hashsign)\n",
    "        if remove_stopwords:\n",
    "            meaningful_words = remove_from_list_of_words(tweet_words, english_stopwords)\n",
    "            preprocessed.append(\" \".join(meaningful_words))\n",
    "        else:\n",
    "            preprocessed.append(\" \".join(tweet_words))\n",
    "\n",
    "    return preprocessed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Сега да се навратиме на листата од твитови која ја прочитавме од датотеката за тренирање, и листата од твитови ја чистиме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clean tweets:  4954\n",
      "Example clean tweets: \n",
      "\t we think its much better than kids who obsess over people like jayz amp miley cyrus\n",
      "\t i didnt think this was possible but my apple watch has battery left right now amp i last took it off the charger tuesday around am\n",
      "\t bernie sanders will stop in greensboro nc sunday hes holding a rally at the greensboro coliseum #vote2016\n",
      "\t lets see how many couples are going to start dry humping when eric church sings wrecking ball tomorrow #countryjam\n",
      "\t oh shit might be going to ihop tomorrow elliott gone kill me\n"
     ]
    }
   ],
   "source": [
    "# tweets are cleaned (links removed, stopwords, numbers and special characters)\n",
    "# returns a list of clean tweets, but each tweet is represented as a string not a list of words\n",
    "clean_tweet_words = pre_process_tweets_bag_of_words(tweets_and_class_tuple[0],\n",
    "                                                    remove_stopwords=remove_stopwords,\n",
    "                                                    retain_letters_only=retain_letters_only)\n",
    "print(\"Number of clean tweets: \", len(clean_tweet_words))\n",
    "print(\"Example clean tweets: \")\n",
    "for tweet in clean_tweet_words[0:5]:\n",
    "    print('\\t', tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Од чистите твитови ќе изградиме речник, користејќи го CountVectorizer од sklearn пакетот"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градење на модел - Non Deep Learning Bag of Words\n",
    "Еден од моделите кои ги изградивме се базира на пристапот Bag-of-Words. Сите твитови кои ги имаме за тренирање, по нивното чистење опишано во секција 3, ги искористивме за да изградиме речник на најчести зборови. Од тестирањето, според големината на тренинг множеството (~5000 твитови) не добивме зголемување на точноста за речник со повеќе од 5000 зборови. Откако речникот е генериран, зборовите во него се подредуваат, потоа секој од твитовите се претставува во форма на вектор. Доколку во речникот има n зборови, секој од твитовите се трансформира во вектор со n членови, на секоја позиција се става бројот колку пати се појавува одреден збор од речникот во текстот.\n",
    "Пример, нека речникот ги содржи зборовите: [една, јаболка, портокал, мајка, татко].\n",
    "Текстот „мајка ми изеде една јаболка“ би се трансформирал во векторот (1,1,0,1,0), а текстот „портокал, портокал, сакам портокал“ би бил (0,0,3,0,0). За креирање на речникот на најчести зборови и трансформација на твитовите го користевме пакетот sklearn во Python, или поконретно CountVectorizer\n",
    "\n",
    "Вака добиените вектори за твитовите заедно со класите на кои припаѓа секој од нив (positive, negative, neutral) се предаваат на класификатор на тренирање со што се добива истрениран модел. Класификатори кои ги испробавме се: RandomForestClassifier, SVM, LogisticRegression, ExtraTreesClassifier. Сите имплементации ги користевме од sklearn пакетот. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words ...\n",
      "Vocabulary is complete. \n",
      "Example words: \n",
      "aapl\n",
      "abc\n",
      "abigail\n",
      "able\n",
      "about\n",
      "above\n",
      "absence\n",
      "absolute\n",
      "absolutely\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"Creating the bag of words ...\")\n",
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words=None,\n",
    "                             max_features=5000)\n",
    "\n",
    "# each tweet is converted to a vector using the bag of words method\n",
    "train_data_features = vectorizer.fit_transform(clean_tweet_words)\n",
    "train_data_features = train_data_features.toarray()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Vocabulary is complete. \")\n",
    "print(\"Example words: \")\n",
    "for word in vocab[1:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the classifier, if svm will take a while\n",
      "Classifier finished training\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm as svm\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=200)\n",
    "# classifier = ExtraTreesClassifier(n_estimators=200, n_jobs=-1, verbose=False,\n",
    "#                                   class_weight='auto',\n",
    "#                                   bootstrap=True)\n",
    "# classifier = LogisticRegression(verbose=False)\n",
    "print(\"Training the classifier, if svm will take a while\")\n",
    "# max_iterations = 10 #for dev purposes, set -1 for infinity\n",
    "classifier = classifier.fit(train_data_features, tweets_and_class_tuple[1])\n",
    "print(\"Classifier finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откако ќе се истренира класификаторот, треба да ги вчитаме податоците од тест множеството, и нив како и тренинг множеството ги обработуваме. Прво мора да дефинираме една нова функција која од тест множеството ќе направи торка од твитовите и нивните редни броеви"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tuple_from_test_input_file(tweets_with_number, delimiter):\n",
    "    \"\"\"splits each tuple (index, tweet) adding the results in into tweets and indexes returns (tweets, indexes)\"\"\"\n",
    "    tweets = []\n",
    "    index_numbers = []\n",
    "    for tweet in tweets_with_number:\n",
    "        splits = tweet.split(delimiter)\n",
    "        tweets.append(splits[1])\n",
    "        index_numbers.append(splits[0])\n",
    "\n",
    "    return tweets, index_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ја читаме датотеката со тест множеството, ги чистиме твитовите со истите поставки како и тренинг множеството"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning test tweets\n",
      "Number of clean tweets:  32009\n"
     ]
    }
   ],
   "source": [
    "test_lines = []\n",
    "with open(test_file_name) as f:\n",
    "    test_lines = f.readlines()\n",
    "\n",
    "print(\"Cleaning test tweets\")\n",
    "# convert from (index, tweet)[] to (index[], tweet[])\n",
    "test_tweets_and_index = get_tuple_from_test_input_file(test_lines, \"\\t\")\n",
    "clean_test_tweet_words = pre_process_tweets_bag_of_words(test_tweets_and_index[0],\n",
    "                                                         remove_stopwords=remove_stopwords,\n",
    "                                                         retain_letters_only=retain_letters_only)\n",
    "print(\"Number of clean tweets: \", len(clean_test_tweet_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потоа со постоечкиот речник ги претвараме тест твитовите во торба од зборови и го користиме класификаторот што претходно го тренивравме за да направивме предвидувања"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction\n",
      "Predicition done\n"
     ]
    }
   ],
   "source": [
    "# create a vector representation of the test tweets\n",
    "test_data_features = vectorizer.transform(clean_test_tweet_words)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "print(\"Starting prediction\")\n",
    "result = classifier.predict(test_data_features)\n",
    "print(\"Predicition done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следно ја читаме датотеката со решенијата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solutions = []\n",
    "with open(solutions_file_name) as f:\n",
    "    solutions = f.readlines()\n",
    "\n",
    "solution_and_index = get_tuple_from_test_input_file(solutions, \"\\t\")\n",
    "solutions = solution_and_index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За одредување на перформансите, (точноста) на класифицирањето ќе ја користиме истата мерка дефинирана во SemEval натпреаварот[1]. \n",
    "Мерката се дефинира како:\n",
    "![f1](f1_pn.png \"F1_pn\")\n",
    "F<sub>1</sub><sup>P</sup> е оцената за класата POSITIVE:\n",
    "![f1_p](f1_p.png \"F1_p\")\n",
    "Овде $\\pi$<sup>P</sup> и $\\rho$<sup>P</sup> ги означуваат precision и recall за POSITIVE класата, или:\n",
    "![pi_p](pi_p.png \"Pi_p\")\n",
    "![ro_p](ro_p.png \"Ro_p\")\n",
    "Каде PP, UP, NP, PU, PN се ќелиите во следнава конфузиона матрица:\n",
    "\n",
    "|            |          |          |Вистинска|          |\n",
    "|-----------:|---------:|---------:|--------:|---------:|\n",
    "|            |          | POSITIVE | NEUTRAL | NEGATIVE |\n",
    "|            | POSITIVE |    PP    |    PU   |    PN    |\n",
    "| Предвидена | NEUTRAL  |    UP    |    UU   |    UN    |\n",
    "|            | NEGATIVE |    NP    |    NU   |    NN    |\n",
    "\n",
    "Ќелија XY oд конфузионата матрица означува „број на твитови што класификаторот ги препознал како X, а вистинската класа била Y“. Аналогно на F<sub>1</sub><sup>P</sup> се дефинира и F<sub>1</sub><sup>N</sup>. \n",
    "За споредба ќе ги користиме резултатите од SemEval2016, за истата задача дадени во [1] каде најдобриот тим има добиено резултат F<sub>1</sub><sup>PN</sup> = 0.633, а најслабиот тим е со резултат F<sub>1</sub><sup>PN</sup> = 0.303\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POSITIVE = \"positive\"\n",
    "NEGATIVE = \"negative\"\n",
    "NEUTRAL = \"neutral\"\n",
    "\n",
    "\n",
    "def calculate_score(solutions, result):\n",
    "    if len(solutions) != len(result):\n",
    "        raise Exception(\n",
    "            \"solutions and result are not the same length, \" + str(len(solutions)) + \" vs \" + str(len(result)))\n",
    "\n",
    "    confusion_matrix = {}\n",
    "\n",
    "    total = len(solutions)\n",
    "    for i in range(0, total):\n",
    "        result_class = result[i].strip()\n",
    "        if result_class not in confusion_matrix:\n",
    "            confusion_matrix[result_class] = {}\n",
    "\n",
    "        result_row = confusion_matrix[result_class]\n",
    "        solution_class = solutions[i].strip()\n",
    "        if solution_class not in result_row:\n",
    "            result_row[solution_class] = 0.0\n",
    "\n",
    "        result_row[solution_class] += 1.0\n",
    "\n",
    "    PP = confusion_matrix[POSITIVE][POSITIVE]\n",
    "    PU = confusion_matrix[POSITIVE][NEUTRAL]\n",
    "    PN = confusion_matrix[POSITIVE][NEGATIVE]\n",
    "    UP = confusion_matrix[NEUTRAL][POSITIVE]\n",
    "    NP = confusion_matrix[NEGATIVE][POSITIVE]\n",
    "    NN = confusion_matrix[NEGATIVE][NEGATIVE]\n",
    "    NU = confusion_matrix[NEGATIVE][NEUTRAL]\n",
    "    UN = confusion_matrix[NEUTRAL][NEGATIVE]\n",
    "\n",
    "    precision_p = PP / (PP + PU + PN)\n",
    "    recall_p = PP / (PP + UP + NP)\n",
    "    F1_P = (2 * precision_p * recall_p) / (precision_p + recall_p)\n",
    "\n",
    "    precision_n = NN / (NP + NU + PN)\n",
    "    recall_n = NN / (NN + UN + PN)\n",
    "    F1_N = (2 * precision_n * recall_n) / (precision_n + recall_n)\n",
    "    return (F1_P + F1_N) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'positive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fea1a2395208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolutions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calculated F1 score: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-d6a6c714e000>\u001b[0m in \u001b[0;36mcalculate_score\u001b[0;34m(solutions, result)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mPN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPOSITIVE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNEGATIVE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mUP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNEUTRAL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPOSITIVE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mNP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNEGATIVE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPOSITIVE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNEGATIVE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNEGATIVE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mNU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNEGATIVE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNEUTRAL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'positive'"
     ]
    }
   ],
   "source": [
    "f1 = calculate_score(solutions, result)\n",
    "print(\"Calculated F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Резултати од Non Deel Learning - Bag of Words\n",
    "\n",
    "|       Класификатор      |  Стоп-зборови |F<sub>1</sub><sup>PN</sup> |\n",
    "|------------------------:|--------------:|--------------------------:|\n",
    "|      Random Forest      |  отстранети   |          0.2871           |\n",
    "|      Random Forest      | неотстранети  |          0.2786           |\n",
    "| Extra Trees Classifier  |  отстранети   |          0.297            |\n",
    "| Extra Trees Classifier  | неотстранети  |          0.2966           |\n",
    "|   Logistic Regression   |  отстранети   |          0.4024           |\n",
    "|   Logistic Regression   | неотстранети  |          0.3852           |\n",
    "|           SVM           |  отстранети   |          0.4293           |\n",
    "|           SVM           | неотстранети  |          0.5494           |\n",
    "\n",
    "### Градење на модел - Deep Learning - Bag of Centroids\n",
    "\n",
    "Тим од истражувачи од Google предводени од Томас Миколов имаат развиено архитектура за креирање на векторска репрезентација на зборови од големи множества на реченици. Со нивната архитектура се добиваат модели кои успеваат семантички и синтактички слични зборови да ги престават со просторно блиски вектори [5, 6]. Постои библиотека која претставува имплементација на оваа архитектура, која за Python може да се најде во пакетот gensim [7,8].\n",
    "\n",
    "Ќе ја искористиме идејата дадена во Word2Vec NLP Tutorial на Kaggle[9]. \n",
    "Прво потребно е да се изгради моделот на Word2Vec. Се обидовме да изградиме Word2Vec модел со твитовите кои ги имаме на располагање, но тие беа мало множество и моделот не покажуваше добри карактеристики (не наоѓаше никаква семантичка сличност). Затоа зедовме веќе изграден модел кој се користи во туторијалот на Kaggle. Моделот таму е изграден со користење на податочно множество на коментари и рецензии на филмови на англиски јазик. \n",
    "\n",
    "Бидејќи зборовите во моделот во суштина се само вектори, можеме едноставно да ги кластерираме. Го користиме k-means за да ги добиеме кластерите, притоа одбираме голем број на кластери со релативно малку зборови во кластерите. `Број на кластери = Број на зборови во моделот / 5`. Вака добиените кластери ги користиме за да креираме торба од центроиди. За секој од твитовите наоѓаме секој од зборовите во кој од креираните кластери би припаѓал. И добиваме векторска репрезентација за секој твит. Понатаму со оваа репрезентација на твитовите можеме да тренираме било каков класификатор.\n",
    "\n",
    "Направивме две варијанти во претпроцесирањето така што од хаштазите ги тргавме # карактерите, бидејќи ниеден хаштаг не е претставен во Word2Vec моделот, но сакавме таму каде хаштазите се вистински зборови да ја искористиме нивната информација. \n",
    "\n",
    "Како и за претходниот модел ги специфицираме знаменцата кои ги дефинираат различните начини на претпроцесирање, и кои се тест и тренинг датотеките. Воедно го вчитуваме и Word2Vec моделот, потоа ги прочистуваме твитовите како и претходно. Мала варијација постои во начинот кој е потребен да се спремаат твитовите"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweets_to_wordlist(tweets, remove_stopwords=False, retain_letters_only=True, remove_tarabas=False):\n",
    "    \"\"\"takes a list of tweets, cleans each tweet, but returns a list of tweetwords\n",
    "    tweets[]=> (words[])[]\"\"\"\n",
    "    tweets_with_no_links = remove_links_from_tweets(tweets)\n",
    "    preprocessed = []\n",
    "    english_stopwords = set(stopwords.words(\"english\"))\n",
    "    for tweet in tweets_with_no_links:\n",
    "        tweet_wordlist = tweet_to_list_of_words(tweet, retain_letters_only, remove_tarabas)\n",
    "        if remove_stopwords:\n",
    "            meaningful_words = remove_from_list_of_words(tweet_wordlist, english_stopwords)\n",
    "            preprocessed.append(meaningful_words)\n",
    "        else:\n",
    "            preprocessed.append(tweet_wordlist)\n",
    "\n",
    "    return preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input from:  train_and_dev_data/tweet_input/train_input.tsv\n",
      "Cleaning the training tweets\n",
      "Training tweets are clean\n",
      "Cleaning test tweets\n",
      "Test tweets are clean\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "input_lines = []\n",
    "file_name = \"train_and_dev_data/tweet_input/train_input.tsv\"\n",
    "test_file_name = \"train_and_dev_data/tweet_input/test_input.tsv\"\n",
    "solutions_file_name = \"train_and_dev_data/tweet_output/test_solutions.tsv\"\n",
    "remove_stopwords = False\n",
    "retain_letters_only = True\n",
    "remove_tarabas = True\n",
    "\n",
    "# word2vecModel = Word2Vec.load(\"train_and_dev_data/word2vec_model/300features_40minwords_10context\")\n",
    "\n",
    "print(\"Reading input from: \", file_name)\n",
    "with open(file_name) as f:\n",
    "    input_lines = f.readlines()\n",
    "\n",
    "tweets_and_class_tuple = get_tuple_from_input_file(input_lines, \"\\t\")\n",
    "\n",
    "print(\"Cleaning the training tweets\")\n",
    "clean_tweets = tweets_to_wordlist(\n",
    "    tweets_and_class_tuple[0],\n",
    "    remove_stopwords=remove_stopwords,\n",
    "    retain_letters_only=retain_letters_only,\n",
    "    remove_tarabas=remove_tarabas)\n",
    "print(\"Training tweets are clean\")\n",
    "\n",
    "test_lines = []\n",
    "with open(test_file_name) as f:\n",
    "    test_lines = f.readlines()\n",
    "\n",
    "print(\"Cleaning test tweets\")\n",
    "test_tweets_and_index = get_tuple_from_test_input_file(test_lines, \"\\t\")\n",
    "clean_test_tweets = tweets_to_wordlist(\n",
    "    test_tweets_and_index[0],\n",
    "    remove_stopwords=remove_stopwords,\n",
    "    retain_letters_only=retain_letters_only,\n",
    "    remove_tarabas=remove_tarabas)\n",
    "\n",
    "print(\"Test tweets are clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кластерирањето ќе го направиме со k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word_vectors = word2vecModel.syn0\n",
    "# num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "#verbose output is on, number of jobs = as many cores available\n",
    "# kmeans_clustering = KMeans(n_clusters=num_clusters, verbose=1, n_jobs=-1)\n",
    "# print(\"Starting KMeans\")\n",
    "# idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "# print(\"KMeans done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откако се готови кластерите, потребно е секој од тренинг и тест твитовите да ги претставиме како bag-of-centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count\n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word_centroid_map = dict(zip(word2vecModel.index2word, idx))\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "idx = pickle.load(open(\"train_and_dev_data/idx\", \"rb\"))\n",
    "index2word = pickle.load(open(\"train_and_dev_data/index2word\", \"rb\"))\n",
    "word_centroid_map = dict(zip(index2word, idx))\n",
    "num_clusters = num_centroids = max(word_centroid_map.values()) + 1\n",
    "train_centroids = np.zeros((len(clean_tweets), num_clusters),\n",
    "                           dtype=\"float32\")\n",
    "\n",
    "# transform the training set tweets into bags of centroids\n",
    "counter = 0\n",
    "for tweet in clean_tweets:\n",
    "    train_centroids[counter] = create_bag_of_centroids(tweet, word_centroid_map)\n",
    "    counter += 1\n",
    "\n",
    "# repeat for test tweets\n",
    "test_centroids = np.zeros((len(clean_test_tweets), num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for tweet in clean_test_tweets:\n",
    "    test_centroids[counter] = create_bag_of_centroids(tweet, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потоа тренираме класификатор со тренинг твитовите, овој пат претставени преку центроидите, и потоа го тестираме класификаторот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the classifier\n",
      "Training done\n",
      "Starting prediction\n",
      "Prediction done, starting evaluation\n",
      "Precision:  0.4678386343076686\n"
     ]
    }
   ],
   "source": [
    "# fit a svm and extract predictions\n",
    "print(\"Training the classifier\")\n",
    "classifier = svm.SVC(kernel='rbf', C=10, gamma=0.001, max_iter=-1,\n",
    "                     verbose=False, class_weight='balanced', cache_size=4000,\n",
    "                     probability=True)\n",
    "classifier = classifier.fit(train_centroids, tweets_and_class_tuple[1])\n",
    "print(\"Training done\")\n",
    "\n",
    "print(\"Starting prediction\")\n",
    "result = classifier.predict(test_centroids)\n",
    "\n",
    "print(\"Prediction done, starting evaluation\")\n",
    "solutions = []\n",
    "with open(solutions_file_name) as f:\n",
    "    solutions = f.readlines()\n",
    "\n",
    "solution_and_index = get_tuple_from_test_input_file(solutions, \"\\t\")\n",
    "solutions = solution_and_index[0]\n",
    "precision = calculate_score(solutions, result)\n",
    "print(\"Precision: \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добивањето на резултати со овој метод трае значително многу затоа што процесот на кластерирање на зборовите ов Word2Vec моделот е доста бавен, (неколку саати). Може да се забрза доколку се зачува/серијализира структурата на кластерите. \n",
    "Откако ги добивме репрезентациите на твитовите во векторска форма така што секој од зборовите беше означен на кој кластер припаѓа искористивме SVM класификатор. Стоп зборовите не беа отстранети, само буквите беа заддржани и линковите отстранети. \n",
    "\n",
    "Без отстранување на тараба карактерите од хаштазите се доби резултат F<sub>1</sub><sup>PN</sup> = 0.474, а ако се отстранат тогаш се добива F<sub>1</sub><sup>PN</sup> = 0.467."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Референци\n",
    "\n",
    "1. Nakov, Preslav, et al. \"SemEval-2016 task 4: Sentiment analysis in Twitter.\" Proceedings of SemEval (2016): 1-18. \n",
    "2. Semeval Twitter data download script, https://github.com/aritter/twitter_download\n",
    "3. SemEval-2016 Task 4 test datasets, zip, 3.4 MB, http://alt.qcri.org/semeval2016/task4/data/uploads/semeval2016_task4_test_datasets.zip\n",
    "4. SemEval-2016 Task 4 Train, Dev and DevTest data, zip, 6.8 MB http://alt.qcri.org/semeval2016/task4/data/uploads/semeval2016-task4.traindevdevtest.v1.2.zip\n",
    "5. Mikolov, Tomas, et al. \"Efficient estimation of word representations in vector space.\" arXiv preprint arXiv:1301.3781 (2013).\n",
    "6. Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing systems. 2013.\n",
    "7. Gensim, topic modeling for humans, website: https://radimrehurek.com/gensim/\n",
    "8. Python Framework for fast vector space modeling, PyPi package https://pypi.python.org/pypi/gensim\n",
    "9. https://www.kaggle.com/c/word2vec-nlp-tutorial\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
