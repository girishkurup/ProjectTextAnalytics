{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - using WordNet PosTag Lemmatization\n",
    "\n",
    "Ќе го модификуваме претходното решение малку, со тоа што ќе се обидеме да направиме лематизација \n",
    "на зборовите користејќи ја WordNet мрежата.\n",
    "\n",
    "Но за да разликуваме синоними кои имаат различна природа (глагол наспроти именка и сл.) ќе се обидеме прво на секој\n",
    "од зборовите да му ја најдеме улогата во реченицата, т.е. PosTag-от со nltk. \n",
    "\n",
    "Зависно од типот на тагот ќе имаме 3 категории на зборови:\n",
    "* Зборови кои ќе се игнорираат (испуштаат)\n",
    " * CC, DT, EX, LS, PDT, POS, RP, UH, WDT, WP, WP\\$, WRB, MD\n",
    "* Зборови кои ќе се остават какви што се\n",
    " * CD, FW, IN, PRP, PRP\\$\n",
    "* Зборови кои ќе се лематизираат\n",
    " * JJ, JJR, JJS, NN, NNS, NNPS, RB, RBR, RBS, VB, VBD, VBG, VBN, VBP, VBZ\n",
    " \n",
    " За што означува секој од таговите повеќе на следниов [линк](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatization with wordnet and lexicon creation\n",
    "POS_TAGS_TO_IGNORE = ['CC', 'DT', 'EX', 'LS', 'PDT', 'POS', 'RP', 'UH', 'WDT', 'WP', 'WP$', 'WRB', 'MD']\n",
    "POS_TAGS_TO_LEAVE_AS_IS = ['CD', 'FW', 'IN', 'PRP', 'PRP$']\n",
    "POS_TAGS_TO_LEMMATIZE = ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP',\n",
    "                         'VBZ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PосTag-ерот на nltk има многу категории, но WordNet разликува само 4 категории, придавки (adj), глаголи (verb), именки (noun) и прилози (adv)\n",
    " \n",
    "Конверзијата меѓу двата вида тагови ја правиме со"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(pos_tag: str):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return nltk.corpus.reader.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return nltk.corpus.reader.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return nltk.corpus.reader.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return nltk.corpus.reader.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пред да лематизираме проверуваме дали пос тагот е погоден за ворднет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def lemmatize_with_wordnet(word: str, pos_tag: str, lemmatizer: WordNetLemmatizer):\n",
    "    wordnet_pos = get_wordnet_pos(pos_tag)\n",
    "    if wordnet_pos is not None:\n",
    "        return lemmatizer.lemmatize(word, wordnet_pos)\n",
    "\n",
    "    return word\n",
    "\n",
    "# end of lexicon creation and lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фунцкиите кои листа од твитови ја лематизираат и токенизираат се дадени следни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prune_tokens_based_on_pos(word: str, pos_tag: str, lemmatizer: WordNetLemmatizer):\n",
    "    if pos_tag in POS_TAGS_TO_IGNORE:\n",
    "        return None\n",
    "    elif pos_tag in POS_TAGS_TO_LEAVE_AS_IS:\n",
    "        return word\n",
    "    elif pos_tag in POS_TAGS_TO_LEMMATIZE:\n",
    "        return lemmatize_with_wordnet(word, pos_tag, lemmatizer)\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "def prune_token_list(tokens: List[str], lemmatizer: WordNetLemmatizer) -> List[List[str]]:\n",
    "    tokens_with_pos = nltk.pos_tag(tokens)\n",
    "    pruned = [prune_tokens_based_on_pos(token[0], token[1], lemmatizer) for token in tokens_with_pos]\n",
    "    return list(filter(lambda x: x is not None, pruned))\n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize_tweets(tweets: List[str]) -> List[List[str]]:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    pruned_tweets = []\n",
    "    count = 0\n",
    "    for tweet in tweets:\n",
    "        count += 1\n",
    "        tokenized_tweet = nltk.word_tokenize(tweet.lower())\n",
    "        pruned_tweet = prune_token_list(tokenized_tweet, lemmatizer)\n",
    "        pruned_tweets.append(pruned_tweet)\n",
    "        if count % 1000 == 0:\n",
    "            print(\"Processed tweets: \",count)\n",
    "            \n",
    "    return pruned_tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример како се користи функцијата, и какви резултати дава:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example usage:\n",
      "[['i', 'like', 'to', 'eat', 'pie']]\n",
      "[['eat', 'pie', 'be', 'extremely', 'difficult']]\n",
      "[['be', 'you', 'my', 'friend', '?', '-fine', ',', 'thanks', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example usage:\")\n",
    "print(tokenize_and_lemmatize_tweets([\"i like to eat pie\"]))\n",
    "print(tokenize_and_lemmatize_tweets([\"eating pie can be extremely difficult\"]))\n",
    "print(tokenize_and_lemmatize_tweets([\"How are you my friend? -Fine, thanks.\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И овој пат ќе ги отстраниме линковите од твитовите, со истата функција од претходно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_pattern_from_string(given_string, re_compiled_pattern):\n",
    "    \"\"\"removes a re compiled regex pattern from a given string \"\"\"\n",
    "    return re_compiled_pattern.sub(\"\", given_string)\n",
    "\n",
    "def remove_links_from_tweets(tweets_list: List[str]) -> List[str]: \n",
    "    \"\"\"for a given list of tweet texts removes all links starting with http:// or https://\"\"\"\n",
    "    regex = re.compile(\"(https?://\\S+)\", re.IGNORECASE)\n",
    "    tweets_without_links = []\n",
    "    for tweet in tweets_list:\n",
    "        clean_tweet = remove_pattern_from_string(tweet, regex)\n",
    "        tweets_without_links.append(clean_tweet)\n",
    "    return tweets_without_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сакаме да се обидеме зборовите кои воопшто не се во WordNet да ги игнорираме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', 'pie', '#fire', 'bableh', '3rd']  ->  ['i', 'like', 'pie', 'fire']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "letters_only_regex = re.compile(\"[^a-zA-Z]\")\n",
    "\n",
    "def keep_only_wordnet_tokens(tweet_tokens:List[str]) -> List[str]:\n",
    "    tweet_tokens = [letters_only_regex.sub(\"\", word) for word in tweet_tokens]\n",
    "    return [word for word in tweet_tokens if wordnet.synsets(word)]\n",
    "    \n",
    "example_tokens = ['i', 'like', 'pie', '#fire', 'bableh','3rd']\n",
    "pruned_tokens = keep_only_wordnet_tokens(example_tokens)\n",
    "print(example_tokens, ' -> ', pruned_tokens)\n",
    "\n",
    "def keep_only_wordnet_tokens_in_list_of_tweets(tweets: List[List[str]]) -> List[List[str]]:\n",
    "    return [keep_only_wordnet_tokens(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сите трансформации на твитовите ги комбинираме во една функција"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_words_in_list_of_tweets(tweet_list : List[List[str]]) -> List[str]:\n",
    "    return [\" \".join(tweet) for tweet in tweet_list]\n",
    "\n",
    "def clean_list_of_tweets(tweet_list:List[str], keep_only_wordnet_tokens: bool = True) -> List[str]:\n",
    "    modified_tweets = remove_links_from_tweets(tweet_list)\n",
    "    modified_tweets = tokenize_and_lemmatize_tweets(modified_tweets)\n",
    "    if keep_only_wordnet_tokens:\n",
    "        modified_tweets = keep_only_wordnet_tokens_in_list_of_tweets(modified_tweets)\n",
    "    modified_tweets = join_words_in_list_of_tweets(modified_tweets)\n",
    "    return modified_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Како и претходно ги вчитуваме сите твитови од влезната датотека и ги конвертираме во погоден формат за пречистување"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_lines = []\n",
    "\n",
    "# training data set, each line = (tweet, class)\n",
    "train_file_name = \"train_and_dev_data/tweet_input/train_input.tsv\"\n",
    "\n",
    "# test data set, each line = (index/ line no., tweet)\n",
    "test_file_name = \"train_and_dev_data/tweet_input/test_input.tsv\"\n",
    "\n",
    "# solutions file, each line = (index, correct class)\n",
    "solutions_file_name = \"train_and_dev_data/tweet_output/test_solutions.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input from:  train_and_dev_data/tweet_input/train_input.tsv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Reading input from: \", train_file_name)\n",
    "with open(train_file_name) as f:\n",
    "    input_lines = f.readlines()\n",
    "\n",
    "def get_tuple_from_input_file(lines_with_tweet_and_class, delimiter):\n",
    "    \"\"\"splits each tuple (tweet, class) and appends them to tweets[] and classes[] accordingly and \n",
    "        returns them as (tweets, classes)\"\"\"\n",
    "    tweets = []\n",
    "    classes = []\n",
    "    for tweet in lines_with_tweet_and_class:\n",
    "        splits = tweet.split(delimiter)\n",
    "        tweets.append(splits[0])\n",
    "        classes.append(splits[1])\n",
    "\n",
    "    return tweets, classes\n",
    "\n",
    "# convert from (tweet, class)[] to (tweet[], class[])\n",
    "tweets_and_class_tuple = get_tuple_from_input_file(input_lines, \"\\t\")\n",
    "clean_tweets = clean_list_of_tweets(tweets_and_class_tuple[0], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tweets:  1000\n",
      "Processed tweets:  2000\n",
      "Processed tweets:  3000\n",
      "Processed tweets:  4000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we think it 's much good than kid obsess over people like jay-z amp ; cyrus .\n"
     ]
    }
   ],
   "source": [
    "print(clean_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words ...\n",
      "Bag of words done, example vocab:  ['00', '000', '00pm', '01', '03', '039', '08', '0800', '09', '10']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"Creating the bag of words ...\")\n",
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words=None,\n",
    "                             max_features=5000)\n",
    "\n",
    "# each tweet is converted to a vector using the bag of words method\n",
    "train_data_features = vectorizer.fit_transform(clean_tweets)\n",
    "train_data_features = train_data_features.toarray()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(\"Bag of words done, example vocab: \",vocab[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методот на креирање на речникот и потоа тренирање на класификаторот остануваат исти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the classifier\n",
      "[LibSVM]Training done\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm as svm\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"Training the classifier\")\n",
    "classifier = svm.SVC(kernel='rbf', C=10, gamma=0.001, max_iter=-1,\n",
    "                     verbose=True, class_weight='balanced', cache_size=4000,\n",
    "                     probability=True)\n",
    "# classifier = RandomForestClassifier(n_estimators=200)\n",
    "# classifier = ExtraTreesClassifier(n_estimators=200, n_jobs=-1, verbose=False,\n",
    "#                                   class_weight='auto',\n",
    "#                                   bootstrap=True)\n",
    "# classifier = LogisticRegression(verbose=False)\n",
    "classifier = classifier.fit(train_data_features, tweets_and_class_tuple[1])\n",
    "\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning test tweets\n",
      "Processed tweets:  1000\n",
      "Processed tweets:  2000\n",
      "Processed tweets:  3000\n",
      "Processed tweets:  4000\n",
      "Processed tweets:  5000\n",
      "Processed tweets:  6000\n",
      "Processed tweets:  7000\n",
      "Processed tweets:  8000\n",
      "Processed tweets:  9000\n",
      "Processed tweets:  10000\n",
      "Processed tweets:  11000\n",
      "Processed tweets:  12000\n",
      "Processed tweets:  13000\n",
      "Processed tweets:  14000\n",
      "Processed tweets:  15000\n",
      "Processed tweets:  16000\n",
      "Processed tweets:  17000\n",
      "Processed tweets:  18000\n",
      "Processed tweets:  19000\n",
      "Processed tweets:  20000\n",
      "Processed tweets:  21000\n",
      "Processed tweets:  22000\n",
      "Processed tweets:  23000\n",
      "Processed tweets:  24000\n",
      "Processed tweets:  25000\n",
      "Processed tweets:  26000\n",
      "Processed tweets:  27000\n",
      "Processed tweets:  28000\n",
      "Processed tweets:  29000\n",
      "Processed tweets:  30000\n",
      "Processed tweets:  31000\n",
      "Processed tweets:  32000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_tuple_from_test_input_file(tweets_with_number, delimiter):\n",
    "    \"\"\"splits each tuple (index, tweet) adding the results in into tweets and indexes returns (tweets, indexes)\"\"\"\n",
    "    tweets = []\n",
    "    index_numbers = []\n",
    "    for tweet in tweets_with_number:\n",
    "        splits = tweet.split(delimiter)\n",
    "        tweets.append(splits[1])\n",
    "        index_numbers.append(splits[0])\n",
    "\n",
    "    return tweets, index_numbers\n",
    "\n",
    "\n",
    "test_lines = []\n",
    "with open(test_file_name) as f:\n",
    "    test_lines = f.readlines()\n",
    "\n",
    "print(\"Cleaning test tweets\")\n",
    "# convert from (index, tweet)[] to (index[], tweet[])\n",
    "test_tweets_and_index = get_tuple_from_test_input_file(test_lines, \"\\t\")\n",
    "\n",
    "clean_test_tweets = clean_list_of_tweets(test_tweets_and_index[0], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction\n",
      "Prediction done, starting evaluation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a vector representation of the test tweets\n",
    "test_data_features = vectorizer.transform(clean_test_tweets)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "print(\"Starting prediction\")\n",
    "result = classifier.predict(test_data_features)\n",
    "\n",
    "\n",
    "print(\"Prediction done, starting evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POSITIVE = \"positive\"\n",
    "NEGATIVE = \"negative\"\n",
    "NEUTRAL = \"neutral\"\n",
    "\n",
    "\n",
    "def calculate_score(solutions, result):\n",
    "    if len(solutions) != len(result):\n",
    "        raise Exception(\n",
    "            \"solutions and result are not the same length, \" + str(len(solutions)) + \" vs \" + str(len(result)))\n",
    "\n",
    "    confusion_matrix = {}\n",
    "\n",
    "    total = len(solutions)\n",
    "    for i in range(0, total):\n",
    "        result_class = result[i].strip()\n",
    "        if result_class not in confusion_matrix:\n",
    "            confusion_matrix[result_class] = {}\n",
    "\n",
    "        result_row = confusion_matrix[result_class]\n",
    "        solution_class = solutions[i].strip()\n",
    "        if solution_class not in result_row:\n",
    "            result_row[solution_class] = 0.0\n",
    "\n",
    "        result_row[solution_class] += 1.0\n",
    "\n",
    "    PP = confusion_matrix[POSITIVE][POSITIVE]\n",
    "    PU = confusion_matrix[POSITIVE][NEUTRAL]\n",
    "    PN = confusion_matrix[POSITIVE][NEGATIVE]\n",
    "    UP = confusion_matrix[NEUTRAL][POSITIVE]\n",
    "    NP = confusion_matrix[NEGATIVE][POSITIVE]\n",
    "    NN = confusion_matrix[NEGATIVE][NEGATIVE]\n",
    "    NU = confusion_matrix[NEGATIVE][NEUTRAL]\n",
    "    UN = confusion_matrix[NEUTRAL][NEGATIVE]\n",
    "\n",
    "    precision_p = PP / (PP + PU + PN)\n",
    "    recall_p = PP / (PP + UP + NP)\n",
    "    F1_P = (2 * precision_p * recall_p) / (precision_p + recall_p)\n",
    "\n",
    "    precision_n = NN / (NP + NU + PN)\n",
    "    recall_n = NN / (NN + UN + PN)\n",
    "    F1_N = (2 * precision_n * recall_n) / (precision_n + recall_n)\n",
    "    return (F1_P + F1_N) / 2\n",
    "\n",
    "solutions = []\n",
    "with open(solutions_file_name) as f:\n",
    "    solutions = f.readlines()\n",
    "\n",
    "solution_and_index = get_tuple_from_test_input_file(solutions, \"\\t\")\n",
    "solutions = solution_and_index[0]\n",
    "f1 = calculate_score(solutions, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4634390115963537\n"
     ]
    }
   ],
   "source": [
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Резултатите се далеку полоши од претходно, ни со менување на тоа дали ќе се земат предвид само зборови кои ги има во WordNet каталогот или не"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
