{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Twitter Sentiment Analysis with PosTag Lemmatization and Neural Net classification\n",
    "\n",
    "Ќе се обидеме да искористиме невронска мрежа наместо едноставен класификатор, претпроцесирањето на твитовите ќе го оставиме како во вториот обид со користење на WordNet лематизација од nltk пакетот. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example usage:\n",
      "[['i', 'like', 'to', 'eat', 'pie']]\n",
      "[['eat', 'pie', 'be', 'extremely', 'difficult']]\n",
      "[['be', 'you', 'my', 'friend', '?', '-fine', ',', 'thanks', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from typing import List\n",
    "\n",
    "# lemmatization with wordnet and lexicon creation\n",
    "\n",
    "POS_TAGS_TO_IGNORE = ['CC', 'DT', 'EX', 'LS', 'PDT', 'POS', 'RP', 'UH', 'WDT', 'WP', 'WP$', 'WRB', 'MD']\n",
    "POS_TAGS_TO_LEAVE_AS_IS = ['CD', 'FW', 'IN', 'PRP', 'PRP$']\n",
    "POS_TAGS_TO_LEMMATIZE = ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP',\n",
    "                         'VBZ']\n",
    "\n",
    "\n",
    "def get_wordnet_pos(pos_tag: str):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return nltk.corpus.reader.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return nltk.corpus.reader.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return nltk.corpus.reader.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return nltk.corpus.reader.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatize_with_wordnet(word: str, pos_tag: str, lemmatizer: WordNetLemmatizer):\n",
    "    wordnet_pos = get_wordnet_pos(pos_tag)\n",
    "    if wordnet_pos is not None:\n",
    "        return lemmatizer.lemmatize(word, wordnet_pos)\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def prune_tokens_based_on_pos(word: str, pos_tag: str, lemmatizer: WordNetLemmatizer):\n",
    "    if pos_tag in POS_TAGS_TO_IGNORE:\n",
    "        return None\n",
    "    elif pos_tag in POS_TAGS_TO_LEAVE_AS_IS:\n",
    "        return word\n",
    "    elif pos_tag in POS_TAGS_TO_LEMMATIZE:\n",
    "        return lemmatize_with_wordnet(word, pos_tag, lemmatizer)\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "def prune_token_list(tokens: List[str], lemmatizer: WordNetLemmatizer) -> List[List[str]]:\n",
    "    tokens_with_pos = nltk.pos_tag(tokens)\n",
    "    pruned = [prune_tokens_based_on_pos(token[0], token[1], lemmatizer) for token in tokens_with_pos]\n",
    "    return list(filter(lambda x: x is not None, pruned))\n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize_tweets(tweets: List[str]) -> List[List[str]]:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    pruned_tweets = []\n",
    "    count = 0\n",
    "    for tweet in tweets:\n",
    "        count += 1\n",
    "        tokenized_tweet = nltk.word_tokenize(tweet.lower())\n",
    "        pruned_tweet = prune_token_list(tokenized_tweet, lemmatizer)\n",
    "        pruned_tweets.append(pruned_tweet)\n",
    "        if count % 1000 == 0:\n",
    "            print(\"Processed tweets: \",count)\n",
    "            \n",
    "    return pruned_tweets\n",
    "\n",
    "# end of lexicon creation and lemmatization\n",
    "\n",
    "print(\"Example usage:\")\n",
    "print(tokenize_and_lemmatize_tweets([\"i like to eat pie\"]))\n",
    "print(tokenize_and_lemmatize_tweets([\"eating pie can be extremely difficult\"]))\n",
    "print(tokenize_and_lemmatize_tweets([\"How are you my friend? -Fine, thanks.\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_pattern_from_string(given_string, re_compiled_pattern):\n",
    "    \"\"\"removes a re compiled regex pattern from a given string \"\"\"\n",
    "    return re_compiled_pattern.sub(\"\", given_string)\n",
    "\n",
    "def remove_links_from_tweets(tweets_list: List[str]) -> List[str]: \n",
    "    \"\"\"for a given list of tweet texts removes all links starting with http:// or https://\"\"\"\n",
    "    regex = re.compile(\"(https?://\\S+)\", re.IGNORECASE)\n",
    "    tweets_without_links = []\n",
    "    for tweet in tweets_list:\n",
    "        clean_tweet = remove_pattern_from_string(tweet, regex)\n",
    "        tweets_without_links.append(clean_tweet)\n",
    "    return tweets_without_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', 'pie', '#fire', 'bableh', '3rd']  ->  ['i', 'like', 'pie', 'fire']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "letters_only_regex = re.compile(\"[^a-zA-Z]\")\n",
    "\n",
    "def keep_only_wordnet_tokens(tweet_tokens:List[str]) -> List[str]:\n",
    "    tweet_tokens = [letters_only_regex.sub(\"\", word) for word in tweet_tokens]\n",
    "    return [word for word in tweet_tokens if wordnet.synsets(word)]\n",
    "    \n",
    "example_tokens = ['i', 'like', 'pie', '#fire', 'bableh','3rd']\n",
    "pruned_tokens = keep_only_wordnet_tokens(example_tokens)\n",
    "print(example_tokens, ' -> ', pruned_tokens)\n",
    "\n",
    "def keep_only_wordnet_tokens_in_list_of_tweets(tweets: List[List[str]]) -> List[List[str]]:\n",
    "    return [keep_only_wordnet_tokens(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tweets_tralala(tweet_list:List[str]) -> List[str]:\n",
    "    modified_tweets = remove_links_from_tweets(tweet_list)\n",
    "    modified_tweets = tokenize_and_lemmatize_tweets(modified_tweets)\n",
    "    modified_tweets = keep_only_wordnet_tokens_in_list_of_tweets(modified_tweets)\n",
    "    return modified_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tuple_from_input_file(lines_with_tweet_and_class, delimiter):\n",
    "    \"\"\"splits each tuple (tweet, class) and appends them to tweets[] and classes[] accordingly and \n",
    "        returns them as (tweets, classes)\"\"\"\n",
    "    tweets = []\n",
    "    classes = []\n",
    "    for tweet in lines_with_tweet_and_class:\n",
    "        splits = tweet.split(delimiter)\n",
    "        tweets.append(splits[0])\n",
    "        classes.append(splits[1])\n",
    "\n",
    "    return tweets, classes\n",
    "\n",
    "\n",
    "def get_tuple_from_test_input_file(tweets_with_number, delimiter):\n",
    "    \"\"\"splits each tuple (index, tweet) adding the results in into tweets and indexes returns (tweets, indexes)\"\"\"\n",
    "    tweets = []\n",
    "    index_numbers = []\n",
    "    for tweet in tweets_with_number:\n",
    "        splits = tweet.split(delimiter)\n",
    "        tweets.append(splits[1])\n",
    "        index_numbers.append(splits[0])\n",
    "\n",
    "    return tweets, index_numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модел на невронската мрежа\n",
    "\n",
    "За да може да креираме невронска мрежа и да ја тренираме, мораме влезот да го претставиме векторски, го користиме принципот на bag-of-centroids кој го опишавме во првиот документ. За креирање и тренирање на невронската мрежа ќе го користиме Tensorflow. Невронската мрежа ќе има 3 слоеви, секој слој по 500 неврони. Влезот ќе претставува вектор од толку елементи колку што има центроиди (кластери) изградени од word2vec моделот. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 3\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "def convert_question_to_vector(word_centroid_map, tweet: List[str]) -> np.ndarray:\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count\n",
    "    # by one\n",
    "    for word in tweet:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_tweets_to_feature_vectors(centroid_map, clean_tweets: List[List[str]]) -> List[np.ndarray]:\n",
    "    return [convert_question_to_vector(centroid_map, tweet) for tweet in clean_tweets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def neural_network_model(data, num_input:int):\n",
    "    print(\"creating network model\")\n",
    "    hidden_1_layer = {'weights': tf.Variable(tf.random_normal([num_input, n_nodes_hl1])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                    'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3, output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input from:  train_and_dev_data/tweet_input/train_input.tsv\n",
      "Processed tweets:  1000\n",
      "Processed tweets:  2000\n",
      "Processed tweets:  3000\n",
      "Processed tweets:  4000\n",
      "Example converted classes:  [[0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def convert_class(class_of_tweet:str) -> List[int]:\n",
    "    if class_of_tweet.strip() == 'positive':\n",
    "        return [1,0,0]\n",
    "    elif class_of_tweet.strip() == 'negative':\n",
    "        return [0,0,1]\n",
    "    return [0,1,0]\n",
    "\n",
    "def convert_classes(classes: List[str]) -> List[List[int]]:\n",
    "    return [convert_class(class_of_tweet) for class_of_tweet in classes]\n",
    "\n",
    "input_lines = []\n",
    "\n",
    "# training data set, each line = (tweet, class)\n",
    "train_file_name = \"train_and_dev_data/tweet_input/train_input.tsv\"\n",
    "\n",
    "# test data set, each line = (index/ line no., tweet)\n",
    "test_file_name = \"train_and_dev_data/tweet_input/test_input.tsv\"\n",
    "\n",
    "# solutions file, each line = (index, correct class)\n",
    "solutions_file_name = \"train_and_dev_data/tweet_output/test_solutions.tsv\"\n",
    "\n",
    "print(\"Reading input from: \", train_file_name)\n",
    "with open(train_file_name) as f:\n",
    "    input_lines = f.readlines()\n",
    "\n",
    "# convert from (tweet, class)[] to (tweet[], class[])\n",
    "tweets_and_class_tuple = get_tuple_from_input_file(input_lines, \"\\t\")\n",
    "clean_tweets = clean_tweets_tralala(tweets_and_class_tuple[0])\n",
    "\n",
    "converted_classes = convert_classes(tweets_and_class_tuple[1])\n",
    "print(\"Example converted classes: \", converted_classes[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За да не мора да правиме кластерирање на центроидите на секое пуштање, ги имаме серијализирано резултатите со пакетот pickle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = pickle.load(open(\"train_and_dev_data/neural_model/idx\", \"rb\"))\n",
    "index2word = pickle.load(open(\"train_and_dev_data/neural_model/index2word\", \"rb\"))\n",
    "word_centroid_map = dict(zip(index2word, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ги конвертираме твитовите од тренинг множеството во вектори и ги спремаме за тренирање на мрежата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(clean_tweets[0:3])\n",
    "train_tweet_features = convert_tweets_to_feature_vectors(word_centroid_map, clean_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Истото го правиме и на тест множеството"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test input from:  train_and_dev_data/tweet_input/test_input.tsv\n",
      "Processed tweets:  1000\n",
      "Processed tweets:  2000\n",
      "Processed tweets:  3000\n",
      "Processed tweets:  4000\n",
      "Processed tweets:  5000\n",
      "Processed tweets:  6000\n",
      "Processed tweets:  7000\n",
      "Processed tweets:  8000\n",
      "Processed tweets:  9000\n",
      "Processed tweets:  10000\n",
      "Processed tweets:  11000\n",
      "Processed tweets:  12000\n",
      "Processed tweets:  13000\n",
      "Processed tweets:  14000\n",
      "Processed tweets:  15000\n",
      "Processed tweets:  16000\n",
      "Processed tweets:  17000\n",
      "Processed tweets:  18000\n",
      "Processed tweets:  19000\n",
      "Processed tweets:  20000\n",
      "Processed tweets:  21000\n",
      "Processed tweets:  22000\n",
      "Processed tweets:  23000\n",
      "Processed tweets:  24000\n",
      "Processed tweets:  25000\n",
      "Processed tweets:  26000\n",
      "Processed tweets:  27000\n",
      "Processed tweets:  28000\n",
      "Processed tweets:  29000\n",
      "Processed tweets:  30000\n",
      "Processed tweets:  31000\n",
      "Processed tweets:  32000\n",
      "[1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Reading test input from: \", test_file_name)\n",
    "with open(test_file_name) as f:\n",
    "    test_input_lines = f.readlines()\n",
    "\n",
    "# convert from (tweet, class)[] to (tweet[], class[])\n",
    "test_tweets_and_class_tuple = get_tuple_from_test_input_file(test_input_lines, \"\\t\")\n",
    "clean_test_tweets = clean_tweets_tralala(test_tweets_and_class_tuple[0])\n",
    "\n",
    "test_tweet_features = convert_tweets_to_feature_vectors(word_centroid_map, clean_test_tweets)\n",
    "\n",
    "solutions = []\n",
    "with open(solutions_file_name) as f:\n",
    "    solutions = f.readlines()\n",
    "\n",
    "solution_and_index = get_tuple_from_test_input_file(solutions, \"\\t\")\n",
    "solutions = solution_and_index[0]\n",
    "\n",
    "converted_solutions = convert_classes(solutions)\n",
    "print(converted_solutions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_neural_network(x, train_data: Tuple[List, List], test_data: Tuple[List, List]):\n",
    "    print(\"training neural network\")\n",
    "    num_input = len(train_data[0][0])\n",
    "    prediction = neural_network_model(x, num_input)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    hm_epochs = 20\n",
    "    pos_class = np.array([1, 0, 0])\n",
    "    neg_class = np.array([0, 0, 1])\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(hm_epochs):\n",
    "            print(\"starting epoch\", epoch)\n",
    "            epoch_loss = 0\n",
    "            for num_batch in range(int(len(train_data[0]) / batch_size)):\n",
    "                epoch_x = train_data[0][num_batch * batch_size: (num_batch + 1) * batch_size]\n",
    "                epoch_y = train_data[1][num_batch * batch_size:(num_batch + 1) * batch_size]\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "\n",
    "            print('Epoch', epoch, ' completed out of ', hm_epochs, '; loss:', epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        incorrect = tf.not_equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        predicted_pos = tf.equal(tf.argmax(prediction, 1), tf.argmax(pos_class, 0))\n",
    "        predicted_neg = tf.equal(tf.argmax(prediction, 1), tf.argmax(neg_class, 0))\n",
    "\n",
    "        tp = tf.logical_and(correct, predicted_pos)\n",
    "        fp = tf.logical_and(incorrect, predicted_pos)\n",
    "        tn = tf.logical_and(correct, predicted_neg)\n",
    "        fn = tf.logical_and(incorrect, predicted_neg)\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        tp_red = tf.reduce_mean(tf.cast(tp, 'float'))\n",
    "        tn_red = tf.reduce_mean(tf.cast(tn, 'float'))\n",
    "        fp_red = tf.reduce_mean(tf.cast(fp, 'float'))\n",
    "        fn_red = tf.reduce_mean(tf.cast(fn, 'float'))\n",
    "        print('Accuracy:', accuracy.eval({x: test_data[0], y: test_data[1]}))\n",
    "        ktp = tp_red.eval({x: test_data[0], y: test_data[1]})\n",
    "        ktn = tn_red.eval({x: test_data[0], y: test_data[1]})\n",
    "        kfp = fp_red.eval({x: test_data[0], y: test_data[1]})\n",
    "        kfn = fn_red.eval({x: test_data[0], y: test_data[1]})\n",
    "\n",
    "        print('TP:', ktp)\n",
    "        print('TN:', ktn)\n",
    "        print('FP:', kfp)\n",
    "        print('FN:', kfn)\n",
    "        prec_p  = (ktp) / (ktp + kfp)\n",
    "        rek_p = (ktp) / (ktp + kfn)\n",
    "        print('Precision P: ', prec_p)\n",
    "        print('Recall P: ', rek_p)\n",
    "        prec_n = (ktn) / (ktn + kfn)\n",
    "        rek_n = (ktn) / (ktn + kfp)\n",
    "        print('Precision N: ', prec_n)\n",
    "        print('Recall N: ', rek_n)\n",
    "        f1_p = 2*prec_p*rek_p/(prec_p+rek_p)\n",
    "        f1_n = 2*prec_n*rek_n/(prec_n+rek_n)\n",
    "        print('F1pn:', (f1_p+f1_n)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4954\n",
      "4954\n",
      "32009\n",
      "32009\n",
      "training neural network\n",
      "creating network model\n",
      "starting epoch 0\n",
      "Epoch 0  completed out of  20 ; loss: 233350.146973\n",
      "starting epoch 1\n",
      "Epoch 1  completed out of  20 ; loss: 129961.278809\n",
      "starting epoch 2\n",
      "Epoch 2  completed out of  20 ; loss: 82637.1984253\n",
      "starting epoch 3\n",
      "Epoch 3  completed out of  20 ; loss: 56088.772522\n",
      "starting epoch 4\n",
      "Epoch 4  completed out of  20 ; loss: 42728.5026245\n",
      "starting epoch 5\n",
      "Epoch 5  completed out of  20 ; loss: 33485.5773926\n",
      "starting epoch 6\n",
      "Epoch 6  completed out of  20 ; loss: 27504.4920349\n",
      "starting epoch 7\n",
      "Epoch 7  completed out of  20 ; loss: 24557.4794846\n",
      "starting epoch 8\n",
      "Epoch 8  completed out of  20 ; loss: 26855.1344891\n",
      "starting epoch 9\n",
      "Epoch 9  completed out of  20 ; loss: 9973.94610405\n",
      "starting epoch 10\n",
      "Epoch 10  completed out of  20 ; loss: 4743.44536918\n",
      "starting epoch 11\n",
      "Epoch 11  completed out of  20 ; loss: 3684.25131381\n",
      "starting epoch 12\n",
      "Epoch 12  completed out of  20 ; loss: 3300.7180016\n",
      "starting epoch 13\n",
      "Epoch 13  completed out of  20 ; loss: 2485.10696757\n",
      "starting epoch 14\n",
      "Epoch 14  completed out of  20 ; loss: 2314.65037675\n",
      "starting epoch 15\n",
      "Epoch 15  completed out of  20 ; loss: 2098.22181308\n",
      "starting epoch 16\n",
      "Epoch 16  completed out of  20 ; loss: 958.203829736\n",
      "starting epoch 17\n",
      "Epoch 17  completed out of  20 ; loss: 252.052098189\n",
      "starting epoch 18\n",
      "Epoch 18  completed out of  20 ; loss: 13.5430203676\n",
      "starting epoch 19\n",
      "Epoch 19  completed out of  20 ; loss: 11.9678071737\n",
      "Accuracy: 0.419944\n",
      "TP: 0.208785\n",
      "TN: 0.0384267\n",
      "FP: 0.288794\n",
      "FN: 0.128495\n",
      "Precision P:  0.419602\n",
      "Recall P:  0.619026\n",
      "Precision N:  0.230208\n",
      "Recall N:  0.117434\n",
      "F1pn: 0.327848620043\n"
     ]
    }
   ],
   "source": [
    "print(len(train_tweet_features))\n",
    "print(len(converted_classes))\n",
    "\n",
    "print(len(test_tweet_features))\n",
    "print(len(converted_solutions))\n",
    "\n",
    "_x = tf.placeholder('float', [None, len(train_tweet_features[0])])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "train_neural_network(_x, (train_tweet_features, converted_classes), (test_tweet_features, converted_solutions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
